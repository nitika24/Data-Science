aurl <- "https://www.imdb.com/title/tt1477834/reviews?ref_=tt_ov_rt"
library(rvest)
library(XML)
library(magrittr)
library(tm)
library(wordcloud)
library(wordcloud2)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
Sys.setlocale('LC_ALL','C')
# IMDBReviews #############################
aurl <- "https://www.imdb.com/title/tt1477834/reviews?ref_=tt_ov_rt"
IMDB_reviews <- NULL
for (i in 1:10){
murl <- read_html(as.character(paste(aurl,i,sep="=")))
rev <- murl %>%
html_nodes(".show-more__control") %>%
html_text()
IMDB_reviews <- c(IMDB_reviews,rev)
}
length(IMDB_reviews)
setwd("D:\\Nitika\\Data Science\\Assignments\\Text Mining")
write.table(IMDB_reviews,"Aquaman.txt",row.names = F)
Aquaman <- read.delim('Aquaman.txt')
str(Aquaman)
View(Aquaman)
# Build Corpus and DTM/TDM
library(tm)
corpus <- Aquaman[-1,]
head(corpus)
class(corpus)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Clean the text
corpus <- tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removePunctuation)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removeNumbers)
inspect(corpus[1:5])
corpus_clean<-tm_map(corpus,stripWhitespace)
inspect(corpus[1:5])
cleanset<-tm_map(corpus,removeWords, stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*','',x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
cleanset<-tm_map(cleanset,removeWords, c('can','film'))
cleanset<-tm_map(cleanset,removeWords, c('movie','movies'))
cleanset <- tm_map(cleanset, gsub,pattern = 'character', replacement = 'characters')
inspect(cleanset[1:5])
cleanset <- tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
#Term Document Matrix :
# Convert the unstructured data to structured data :
tdm <- TermDocumentMatrix(cleanset)
tdm
# the terms indicate that there are 13649 words and 393036 documents(# of tweets) in this TDM
# Sparsity is 97% which indicates that there are lots of zero values.
tdm <- as.matrix(tdm)
tdm[1:10,1:20]
w <- rowSums(tdm)  # provides the no of times a particular word has been used.
w <- subset(w, w>= 50) # Pull words that were used more than 25 times.
barplot(w, las = 2, col = rainbow(50))
# Word Cloud :
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
library(wordcloud2)
w <- data.frame(names(w),w)
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5,
minSize = 1)
letterCloud(w,word = 'A',frequency(5), size=1)
# Sentiment Analysis for tweets:
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
# Read File
IMDB_reviews <- read.delim('Aquaman.TXT')
reviews <- as.character(IMDB_reviews[-1,])
class(reviews)
# Obtain Sentiment scores
s <- get_nrc_sentiment(reviews)
head(s)
reviews[4]
# Watched it!!! And have no words to describe the splendid performance of the cast and how beautifully wan visualised and presented this..This is the true jewel so far in DC universe<U+0001F44D><U+0001F3FB>\nWon't Add any Spoilers<U+0001F60B>..as this is something you should witness"
# on tweet 4, you have 4 for anger, each one work for disgust, fear
# and sadness, 3 for trust , 4 words for negative and 2 positive.
get_nrc_sentiment('splendid')
# Splendid has one Joy and one positive
get_nrc_sentiment('no words') #1 Anger and 1 Negative
barplot(colSums(s), las = 2.5, col = rainbow(10),
ylab = 'Count',main= 'Sentiment scores for IMDB Reviews
for Aquaman')
library(tidyverse)  #to visualize, transform, input, tidy and join data
library(readr)      #to input data from tsv
library(dplyr)      #data wrangling
library(kableExtra) #to create HTML Table
library(DT)         #to preview the data sets
library(lubridate)  #to apply the date functions
library(tm)         #to text mine
library(wordcloud)  #to build word cloud
library(tidytext)   #to text mine
library(rvest)
library(XML)
library(magrittr)
# Amazon Reviews #############################
aurl <- "https://www.amazon.in/Echo-Dot-3rd-Gen-improved/product-reviews/B07PFFMP9P/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"
amazon_reviews <- NULL
for (i in 1:10){
murl <- read_html(as.character(paste(aurl,i,sep="=")))
rev <- murl %>%
html_nodes(".review-text") %>%
html_text()
#rate <- murl %>%
# html_nodes(".review-rating") %>%
#html_text()
amazon_reviews <- c(amazon_reviews,rev)
}
length(amazon_reviews)
setwd("D:\\Nitika\\Data Science\\Assignments\\Text Mining")
write.table(amazon_reviews,"alexa.text",row.names = FALSE)
write.table(amazon_reviews,"alexa.txt",row.names = FALSE)
alexa <- read.delim('alexa.txt')
#Below the location of the positive and negative words
pos = scan('D:\\Nitika\\Data Science\\Assignments\\Text Mining\\positive-words.txt', what='character', comment.char=';')
neg = scan('D:\\Nitika\\Data Science\\Assignments\\Text Mining\\negative-words.txt', what='character', comment.char=';')
str(alexa)
View(alexa)
# Build Corpus and DTM/TDM
library(tm)
corpus <- alexa[-1,]
head(corpus)
class(corpus)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Clean the text
corpus <- tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removePunctuation)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removeNumbers)
inspect(corpus[1:5])
corpus_clean<-tm_map(corpus,stripWhitespace)
inspect(corpus[1:5])
cleanset<-tm_map(corpus,removeWords, stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*','',x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
cleanset<-tm_map(cleanset,removeWords, c('can','film'))
cleanset<-tm_map(cleanset,removeWords, c('movie','movies'))
cleanset <- tm_map(cleanset, gsub,pattern = 'character', replacement = 'characters')
inspect(cleanset[1:5])
inspect(cleanset[1:5])
cleanset <- tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
#Term Document Matrix :
# Convert the unstructured data to structured data :
tdm <- TermDocumentMatrix(cleanset)
tdm
# the terms indicate that there are 13649 words and 393036 documents(# of tweets) in this TDM
# Sparsity is 97% which indicates that there are lots of zero values.
tdm <- as.matrix(tdm)
tdm[1:10,1:20]
w <- rowSums(tdm)  # provides the no of times a particular word has been used.
w <- subset(w, w>= 50) # Pull words that were used more than 25 times.
barplot(w, las = 2, col = rainbow(50))
# Word Cloud :
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
library(wordcloud2)
w <- data.frame(names(w),w)
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5,
minSize = 1)
letterCloud(w,word = 'A',frequency(5), size=1)
# Sentiment Analysis for tweets:
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
# Read File
IMDB_reviews <- read.delim('alexa.TXT')
# Read File
amazon_reviews <- read.delim('alexa.TXT')
reviews <- as.character(amazon_reviews[-1,])
class(reviews)
# Obtain Sentiment scores
s <- get_nrc_sentiment(reviews)
head(s)
reviews[4]
# Watched it!!! And have no words to describe the splendid performance of the cast and how beautifully wan visualised and presented this..This is the true jewel so far in DC universe<U+0001F44D><U+0001F3FB>\nWon't Add any Spoilers<U+0001F60B>..as this is something you should witness"
# on tweet 4, you have 4 for anger, each one work for disgust, fear
# and sadness, 3 for trust , 4 words for negative and 2 positive.
get_nrc_sentiment('splendid')
# Splendid has one Joy and one positive
get_nrc_sentiment('no words') #1 Anger and 1 Negative
barplot(colSums(s), las = 2.5, col = rainbow(10),
ylab = 'Count',main= 'Sentiment scores for IMDB Reviews
for alexa')
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
scores <- laply(sentences, function(sentence, pos.words, neg.words){
sentence <- gsub('[[:punct:]]', "", sentence)
sentence <- gsub('[[:cntrl:]]', "", sentence)
sentence <- gsub('\\d+', "", sentence)
sentence <- tolower(sentence)
word.list <- str_split(sentence, '\\s+')
words <- unlist(word.list)
pos.matches <- match(words, pos.words)
neg.matches <- match(words, neg.words)
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
score <- sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress)
scores.df <- data.frame(score=scores, text=sentences)
return(scores.df)
}
Dataset <- stack
Dataset$text <- as.factor(Dataset$text)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
# Word Cloud :
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
library(wordcloud2)
w <- data.frame(names(w),w)
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5,
minSize = 1)
barplot(colSums(s), las = 2.5, col = rainbow(10),
ylab = 'Count',main= 'Sentiment scores for IMDB Reviews
for alexa')
library(rvest)
library(XML)
library(magrittr)
library(tm)
library(wordcloud)
library(wordcloud2)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
Sys.setlocale('LC_ALL','C')
# IMDBReviews #############################
aurl <- "https://www.imdb.com/title/tt4154796/reviews?ref_=tt_ov_rt"
IMDB_reviews <- NULL
for (i in 1:10){
murl <- read_html(as.character(paste(aurl,i,sep="=")))
rev <- murl %>%
html_nodes(".show-more__control") %>%
html_text()
IMDB_reviews <- c(IMDB_reviews,rev)
}
length(IMDB_reviews)
setwd("D:\\Nitika\\Data Science\\Assignments\\Text Mining")
write.table(IMDB_reviews,"avenger.txt",row.names = F)
avenger <- read.delim('avenger.txt')
str(avenger)
View(avenger)
# Build Corpus and DTM/TDM
library(tm)
corpus <- avenger[-1,]
head(corpus)
class(corpus)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Clean the text
corpus <- tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removePunctuation)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removeNumbers)
inspect(corpus[1:5])
corpus_clean<-tm_map(corpus,stripWhitespace)
inspect(corpus[1:5])
cleanset<-tm_map(corpus,removeWords, stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*','',x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
cleanset<-tm_map(cleanset,removeWords, c('can','film'))
cleanset<-tm_map(cleanset,removeWords, c('movie','movies'))
cleanset <- tm_map(cleanset, gsub,pattern = 'character', replacement = 'characters')
inspect(cleanset[1:5])
cleanset <- tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
#Term Document Matrix :
# Convert the unstructured data to structured data :
tdm <- TermDocumentMatrix(cleanset)
tdm
# the terms indicate that there are 13649 words and 393036 documents(# of tweets) in this TDM
# Sparsity is 97% which indicates that there are lots of zero values.
tdm <- as.matrix(tdm)
tdm[1:10,1:20]
w <- rowSums(tdm)  # provides the no of times a particular word has been used.
w <- subset(w, w>= 50) # Pull words that were used more than 25 times.
barplot(w, las = 2, col = rainbow(50))
# Word Cloud :
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
library(wordcloud2)
w <- data.frame(names(w),w)
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5,
minSize = 1)
letterCloud(w,word = 'A',frequency(5), size=1)
# Sentiment Analysis for tweets:
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
# Read File
IMDB_reviews <- read.delim('avenger.TXT')
reviews <- as.character(IMDB_reviews[-1,])
class(reviews)
# Obtain Sentiment scores
s <- get_nrc_sentiment(reviews)
head(s)
reviews[4]
# Watched it!!! And have no words to describe the splendid performance of the cast and how beautifully wan visualised and presented this..This is the true jewel so far in DC universe<U+0001F44D><U+0001F3FB>\nWon't Add any Spoilers<U+0001F60B>..as this is something you should witness"
# on tweet 4, you have 4 for anger, each one work for disgust, fear
# and sadness, 3 for trust , 4 words for negative and 2 positive.
get_nrc_sentiment('splendid')
# Splendid has one Joy and one positive
get_nrc_sentiment('no words') #1 Anger and 1 Negative
barplot(colSums(s), las = 2.5, col = rainbow(10),
ylab = 'Count',main= 'Sentiment scores for IMDB Reviews
for avenger')
library(rvest)
library(XML)
library(magrittr)
library(tm)
library(wordcloud)
library(wordcloud2)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
Sys.setlocale('LC_ALL','C')
# IMDBReviews #############################
aurl <- "https://www.imdb.com/title/tt6320628/reviews?ref_=tt_ov_rt"
IMDB_reviews <- NULL
for (i in 1:10){
murl <- read_html(as.character(paste(aurl,i,sep="=")))
rev <- murl %>%
html_nodes(".show-more__control") %>%
html_text()
IMDB_reviews <- c(IMDB_reviews,rev)
}
length(IMDB_reviews)
setwd("D:\\Nitika\\Data Science\\Assignments\\Text Mining")
write.table(IMDB_reviews,"spider.txt",row.names = F)
spider <- read.delim('spider.txt')
str(spider)
View(spider)
# Build Corpus and DTM/TDM
library(tm)
corpus <- spider[-1,]
head(corpus)
class(corpus)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Clean the text
corpus <- tm_map(corpus,tolower)
corpus <- tm_map(corpus,removePunctuation)
corpus <- tm_map(corpus,removeNumbers)
corpus_clean<-tm_map(corpus,stripWhitespace)
cleanset<-tm_map(corpus,removeWords, stopwords('english'))
removeURL <- function(x) gsub('http[[:alnum:]]*','',x)
cleanset<-tm_map(cleanset,removeWords, c('can','film'))
cleanset<-tm_map(cleanset,removeWords, c('movie','movies'))
cleanset <- tm_map(cleanset, gsub,pattern = 'character', replacement = 'characters')
inspect(cleanset[1:5])
cleanset <- tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
#Term Document Matrix :
# Convert the unstructured data to structured data :
tdm <- TermDocumentMatrix(cleanset)
tdm
# the terms indicate that there are 13649 words and 393036 documents(# of tweets) in this TDM
# Sparsity is 97% which indicates that there are lots of zero values.
tdm <- as.matrix(tdm)
tdm[1:10,1:20]
w <- rowSums(tdm)  # provides the no of times a particular word has been used.
w <- subset(w, w>= 50) # Pull words that were used more than 25 times.
barplot(w, las = 2, col = rainbow(50))
# Word Cloud :
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
library(wordcloud2)
w <- data.frame(names(w),w)
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5,
minSize = 1)
letterCloud(w,word = 'A',frequency(5), size=1)
# Sentiment Analysis for tweets:
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
# Read File
IMDB_reviews <- read.delim('spider.TXT')
reviews <- as.character(IMDB_reviews[-1,])
class(reviews)
# Obtain Sentiment scores
s <- get_nrc_sentiment(reviews)
head(s)
reviews[4]
# Watched it!!! And have no words to describe the splendid performance of the cast and how beautifully wan visualised and presented this..This is the true jewel so far in DC universe<U+0001F44D><U+0001F3FB>\nWon't Add any Spoilers<U+0001F60B>..as this is something you should witness"
# on tweet 4, you have 4 for anger, each one work for disgust, fear
# and sadness, 3 for trust , 4 words for negative and 2 positive.
get_nrc_sentiment('splendid')
# Splendid has one Joy and one positive
get_nrc_sentiment('no words') #1 Anger and 1 Negative
barplot(colSums(s), las = 2.5, col = rainbow(10),
ylab = 'Count',main= 'Sentiment scores for IMDB Reviews
for spider')
install.packages("C50")
#Installing and loading the libraries
#install.packages("recommenderlab", dependencies=TRUE)
#install.packages("Matrix")
library("recommenderlab")
library(caTools)
#movie rating data
book <- read.csv(file.choose())
book[1]=NULL
View(book)
#metadata about the variable
str(book)
#rating distribution
hist(as.vector(as.matrix(book$Book.Rating)), main = "Distribution of book Ratings",
col = "red", xlab = "Ratings")
boxplot(as.vector(as.matrix(book$Book.Rating)), col = "red", main = "Distribution of book Ratings", ylab = "Ratings")
summary(as.vector(as.matrix(book$Book.Rating)))
#the datatype should be realRatingMatrix inorder to build recommendation engine
book_matrix <- as(book, 'realRatingMatrix')
View(book_matrix)
book_model <- Recommender(book_matrix, method="POPULAR")
#Predictions for two users
recommended_items2 <- predict(book_model, book_matrix[212:214], n=10)
as(recommended_items2, "list")
# non-normalized
UBCF_N_C <- Recommender(book_matrix, "UBCF",
param=list(normalize = NULL, method="Cosine"))
# centered
UBCF_C_C <- Recommender(book_matrix, "UBCF",
param=list(normalize = "center",method="Cosine"))
# Z-score normalization
UBCF_Z_C <- Recommender(book_matrix, "UBCF",
param=list(normalize = "Z-score",method="Cosine"))
# compute predicted ratings
p1 <- predict(UBCF_N_C,book_matrix[200:400], type="ratings")
# compute predicted ratings
p1 <- predict(UBCF_N_C,book_matrix, type="ratings")
